library(lubridate)
library(tidyr)
library(ggplot2)
library(gsheet)
library(tictoc)
library(data.table)
library(dplyr)
library(reticulate)

source("/Volumes/BZ/Scientific Data/RG-AS04-Data01/R_tracking_analysis/scripts/tracking_analysis_functions.R")

## Set the directory to the '_analysis2' folder, or wherever all of the als files are located
setwd("/Volumes/BZ/Scientific Data/RG-AS04-Data01/Hybrid_sleep_videos/_analysis2/")

####### LOAD IN GOOGLE SHEET DATA #######

# The following loads in the meta data from the google sheet. The url is the same as the 'share' link (generated by the share button in google sheets)
# This uses two functions - 'read.csv', which is a base R function, and 'gsheet2text' which comes from package 'gsheet'

url <- 'https://docs.google.com/spreadsheets/d/1qh5uTNiHrI62LEd0wqUqP9yfHQHGlAPfpvCezD43BSY/edit?usp=sharing'
meta_data <- read.csv(text=gsheet2text(url, format='csv', sheetid = 970092928), stringsAsFactors=FALSE)

meta_data$sample_id <- paste("FISH", meta_data$Date..started., "_c", meta_data$Camera, "_r", meta_data$ROI, sep = "")
# You can use 'dim()' or 'head()' to quickly examine the results dataframe to see if it's correct
dim(meta_data)
head(meta_data)

####### IDENTIFY THE FILES FOR IMPORT #######

# List the files in the current directory that are als files
als.files <- list.files(path = ".", recursive = TRUE, pattern = "_als.csv")
als.files.bri <- list.files(path = ".", recursive = TRUE, pattern = "Neolamprologus-brichardi_su_als.csv")
als.files.cra <- list.files(path = ".", recursive = TRUE, pattern = "Neolamprologus-crassus_su_als.csv")


# ####### RUN THE COMMANDS TO IMPORT DATA ########
# 
# # This imports a single file (the 10th file path) and summarises by minute
# als.data <- loadALSfiles(path_to_file = als.files[10], average_by = "minute")
# 
# # This takes a single dataset and further summarises it by halfhour (probably not so useful, but can save space/computation time)
# als.data.2 <- summariseALSdata(als_data = als.data, average_by = "halfhour")
# 
# # Plot all days for an invidual, first set all days to '01'
# als.data.2$daytime <- as.POSIXct(als.data.2$datetime, format = '%Y-%m-%d %H:%M:%S')
# day(als.data.2$daytime) <- 01
# ggplot(als.data.2, aes(x = daytime, y = mean_speed_mm, group = day, colour = day)) + geom_rect_shading_bz() + shade_colours() + geom_point() + geom_line()
# 
# # This averages by day for a single dataset
# # Importantly, ignore the single entry for day 1 when setting 'days_include'. For example, if you want the first 3 days, do 'days_include = c(1,2,3)' NOT 'days_include = c(2,3,4)'
# avg.day <- averageDay(als_data = als.data.2, units = "halfhour", days_include = "all")
# 
# 
# ######## RUN COMMANDS FOR PLOTTING ########
# 
# # Can you '+ geom_rect_shading_bz' or '+ geom_rect_shading_zoo' to add shading based on times
# # Colours can be specified with '+ shade_colours()' which uses grey/yellow/white colour scheme (can make your own)
# 
# ggplot(avg.day, aes(x = datetime, y = mean_speed_mm)) + geom_rect_shading_bz() + shade_colours() + geom_point() + geom_line()



######## COMMANDS FOR MULTIPLE DATASETS & ADDING META_DATA & PLOTTING ########

# This imports a list of files
als.data.list <- lapply(als.files, function(x) loadALSfiles(path_to_file = x, average_by = "minute"))
als.data.list.bri <- lapply(als.files.bri, function(x) loadALSfiles(path_to_file = x, average_by = "minute"))
als.data.list.cra <- lapply(als.files.cra, function(x) loadALSfiles(path_to_file = x, average_by = "minute"))

## Can then save out the files for re-use
saveRDS(als.data.list, file = "als_data_list.rds")

als.data.list <- readRDS("als_data_list.rds")

#### Find peaks

als.data.list.2 <- lapply(als.data.list, function(x) summariseALSdata(als_data = x, average_by = "halfhour"))
als.data.list.bri.2 <- lapply(als.data.list.bri, function(x) summariseALSdata(als_data = x, average_by = "halfhour"))
als.data.list.cra.2 <- lapply(als.data.list.cra, function(x) summariseALSdata(als_data = x, average_by = "halfhour"))

week_peaks_list <- lapply(als.data.list.2, function(x) scipy.signal$find_peaks(x = x$mean_speed_mm, distance = 4, prominence = 7))
week_peaks_list.bri <- lapply(als.data.list.bri.2, function(x) scipy.signal$find_peaks(x = x$mean_speed_mm, distance = 4, prominence = 7))
week_peaks_list.cra <- lapply(als.data.list.cra.2, function(x) scipy.signal$find_peaks(x = x$mean_speed_mm, distance = 4, prominence = 7))

als.data.list.2 <- lapply(seq_along(als.data.list.2), function(x) {
  als.data.list.2[[x]]$peaks <- "no"
  als.data.list.2[[x]]$peaks[week_peaks_list[[x]][[1]]+1] <- "yes"
  als.data.list.2[[x]]$peak_prominence <- NA
  als.data.list.2[[x]]$peak_prominence[week_peaks_list[[x]][[1]]+1] <- week_peaks_list[[x]][[2]]$prominences
  return(als.data.list.2[[x]])
 })

als.data.list.bri.2 <- lapply(seq_along(als.data.list.bri.2), function(x) {
  als.data.list.bri.2[[x]]$peaks <- "no"
  als.data.list.bri.2[[x]]$peaks[week_peaks_list.bri[[x]][[1]]+1] <- "yes"
  als.data.list.bri.2[[x]]$peak_prominence <- NA
  als.data.list.bri.2[[x]]$peak_prominence[week_peaks_list.bri[[x]][[1]]+1] <- week_peaks_list.bri[[x]][[2]]$prominences
  return(als.data.list.bri.2[[x]])
})

als.data.list.cra.2 <- lapply(seq_along(als.data.list.cra.2), function(x) {
  als.data.list.cra.2[[x]]$peaks <- "no"
  als.data.list.cra.2[[x]]$peaks[week_peaks_list.cra[[x]][[1]]+1] <- "yes"
  als.data.list.cra.2[[x]]$peak_prominence <- NA
  als.data.list.cra.2[[x]]$peak_prominence[week_peaks_list.cra[[x]][[1]]+1] <- week_peaks_list.cra[[x]][[2]]$prominences
  return(als.data.list.cra.2[[x]])
})

### Return peak_percentages
percentages <- lapply(als.data.list.2, function(x) returnPeakPercentages(als_data = x))
percentages.bri <- lapply(als.data.list.bri.2, function(x) returnPeakPercentages(als_data = x, zoo_times = TRUE))
percentages.cra <- lapply(als.data.list.cra.2, function(x) returnPeakPercentages(als_data = x, zoo_times = TRUE))

dawn <- c(unlist(lapply(percentages.bri, function(x) x[[1]])), unlist(lapply(percentages.cra, function(x) x[[1]])), unlist(lapply(percentages, function(x) x[[1]])))
dusk <- c(unlist(lapply(percentages.bri, function(x) x[[2]])), unlist(lapply(percentages.cra, function(x) x[[2]])), unlist(lapply(percentages, function(x) x[[2]])))

df <- tibble(dawn_percentages = dawn, dusk_percentages = dusk, sample_id = meta_data$sample_id)
df$percentages <- rowMeans(df[,c("dawn_percentages", "dusk_percentages")])

all_data <- merge(df, meta_data, by = "sample_id")
all_data <- all_data[order(all_data$dawn_percentages),]
all_data$sample_id <- factor(all_data$sample_id, levels = all_data$sample_id)



bar_plot <- ggplot(all_data, aes(x = sample_id, y = dawn_percentages, fill = Generation)) + theme_classic() + geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 90))

x_y <- ggplot(all_data, aes(x = dawn_percentages, y = dusk_percentages, colour = Generation)) + theme_classic() + geom_jitter() + theme(axis.text.x = element_text(angle = 90))
dawn_density <- ggplot(all_data, aes(x = dawn_percentages, colour = Generation)) + theme_classic() + geom_density()
dusk_density <- ggplot(all_data, aes(x = dusk_percentages, colour = Generation)) + theme_classic() + geom_density() + coord_flip()


dawn_density + plot_spacer() + x_y + dusk_density + plot_layout(ncol = 2, guides = "collect")



## This plots the 'peak prominences' from the Python code
all_list <- append(als.data.list.2, als.data.list.bri.2)
all_list <- append(all_list, als.data.list.cra.2)

combined <- Reduce(rbind, all_list)

combined_merge <- merge(combined, meta_data, by = "sample_id")

ggplot(combined_merge, aes(x = peak_prominence, colour = Generation)) + theme_classic() + geom_density()

##### In general, it seems that the F2s and F3s have more peaks (especially at dusk), and their peaks are more prominent, than either parental strain
##### I might be using the wrong times (does 8:30 mean the average half hour between 8:30 - 9:00 or 8:00 -8:30? I need to double check this)


## This is a crude way of plotting which points are 'peaks' according to the algorithm

ggplot(als.data.list.bri.2[[3]], aes(x = datetime, y = mean_speed_mm, color = peaks, group = sample_id)) + geom_rect_shading_bz_7days() + shade_colours() + geom_point() + geom_line(color = "black") + theme_classic()

ggplot(als.data.2, aes(x = daytime, y = mean_speed_mm, group = day, colour = peaks)) + geom_rect_shading_bz() + shade_colours() + geom_point() + geom_line(color = "black") + theme_classic()










# ... and then averages across days by halfhour
avg.day.list <- lapply(als.data.list, function(x) averageDay(als_data = x, units = "halfhour", days_include = "all"))











# Reduce the list into one dataframe
avg.day.combined <- Reduce(rbind, avg.day.list)

# Add meta_data
avg.day.combined <- merge(avg.day.combined, meta_data, by = "sample_id")

## Then plot
plot <- ggplot(avg.day.combined, aes(x = datetime, y = mean_speed_mm, group = sample_id, color = shell)) + geom_rect_shading_bz() + geom_point() + geom_line() + theme_classic()

## Use 'facet_wrap' to separate based on columns in meta_data
## works like an equation ~ means 'by', and you can use addition ('shell+strain' or 'strain+conspecifics')
plot + shade_colours() + facet_wrap(~shell+conspecifics, ncol = 2)


plot + shade_colours() + facet_wrap(~conspecifics, ncol = 2)
plot + shade_colours() + facet_wrap(~shell, ncol = 2)





#### Plot individuals, split by day

# This takes a single dataset and further summarises it by halfhour (probably not so useful, but can save space/computation time)
als.data.2 <- summariseALSdata(als_data = als.data.list[[75]], average_by = "halfhour")

# Plot all days for an invidual, first set all days to '01'
als.data.2$daytime <- as.POSIXct(als.data.2$datetime, format = '%Y-%m-%d %H:%M:%S')
day(als.data.2$daytime) <- 01
ggplot(als.data.2, aes(x = daytime, y = mean_speed_mm, group = day, colour = day)) + geom_rect_shading_bz() + shade_colours() + geom_point() + geom_line()


avg.day <- averageDay(als_data = als.data.2, units = "halfhour", days_include = "all")
ggplot(avg.day, aes(x = datetime, y = mean_speed_mm)) + geom_rect_shading_bz() + shade_colours() + geom_point() + geom_line()


## Run Python functions to find peaks
## Remember, python is 0 indexed, so add 1 to the indices

#scipy.signal <- import("scipy.signal")

week_peaks <- scipy.signal$find_peaks(x = als.data.2$mean_speed_mm, distance=4, prominence=7)
avg_day_peaks <- scipy.signal$find_peaks(x = avg.day$mean_speed_mm, distance=4, prominence=7)

## This is a crude way of plotting which points are 'peaks' according to the algorithm
als.data.2$peaks <- "no"
als.data.2$peaks[week_peaks[[1]]+1] <- "yes"
ggplot(als.data.2, aes(x = datetime, y = mean_speed_mm, color = peaks, group = sample_id)) + geom_rect_shading_bz_7days() + shade_colours() + geom_point() + geom_line(color = "black") + theme_classic()

ggplot(als.data.2, aes(x = daytime, y = mean_speed_mm, group = day, colour = peaks)) + geom_rect_shading_bz() + shade_colours() + geom_point() + geom_line(color = "black") + theme_classic()


avg.day$peaks <- "no"
avg.day$peaks[avg_day_peaks[[1]]+1] <- "yes"
ggplot(avg.day, aes(x = datetime, y = mean_speed_mm, color = peaks, group = sample_id)) + geom_rect_shading_bz() + shade_colours() + geom_point() + geom_line(color = "black") + theme_classic()


## What % of crep periods are there peaks? dawn vs dusk

als.data.2
## Make intervals for each dawn and each dusk
## Then for each, ask if there is a yes?

## for loop to make intervals? 02, 03, 04, 05, 06, 07, 08










